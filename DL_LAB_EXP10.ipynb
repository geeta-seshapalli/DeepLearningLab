{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/MWBH5iUdEt2Hksqe+Cfw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BddkKXlp_2dG","executionInfo":{"status":"ok","timestamp":1745166427350,"user_tz":-330,"elapsed":59120,"user":{"displayName":"Geeta Krishnaveni Seshapalli","userId":"03567840691782124226"}},"outputId":"944385c7-84f3-4be6-d178-c6b9ae1a2470"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step\n","Translated Sentence: merci  \n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","eng_sentences = ['hello', 'how are you', 'thank you']\n","fra_sentences = ['bonjour', 'comment ça va', 'merci']\n","\n","# Tokenization\n","eng_tokenizer = Tokenizer()\n","fra_tokenizer = Tokenizer()\n","eng_tokenizer.fit_on_texts(eng_sentences)\n","fra_tokenizer.fit_on_texts(fra_sentences)\n","\n","eng_seqs = eng_tokenizer.texts_to_sequences(eng_sentences)\n","fra_seqs = fra_tokenizer.texts_to_sequences(fra_sentences)\n","\n","eng_padded = pad_sequences(eng_seqs, padding='post')\n","fra_padded = pad_sequences(fra_seqs, padding='post')\n","\n","# Model parameters\n","embedding_dim = 64\n","units = 128\n","input_vocab_size = len(eng_tokenizer.word_index) + 1\n","target_vocab_size = len(fra_tokenizer.word_index) + 1\n","\n","# Encoder\n","encoder_inputs = Input(shape=(None,))\n","enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n","encoder_lstm = LSTM(units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","dec_emb = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n","decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n","decoder_dense = Dense(target_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Prepare decoder target data\n","decoder_target_data = np.expand_dims(fra_padded, -1)\n","\n","# Train model\n","model.fit([eng_padded, fra_padded], decoder_target_data, batch_size=2, epochs=500, verbose=0)\n","\n","# Simple prediction\n","test_input = pad_sequences(eng_tokenizer.texts_to_sequences(['thank you']), maxlen=3, padding='post')\n","decoder_input = np.zeros((1, 3))  # assume empty start for simplicity\n","preds = model.predict([test_input, decoder_input])\n","predicted_ids = np.argmax(preds[0], axis=-1)\n","\n","# Convert prediction to text\n","reverse_fra_word_index = {i: word for word, i in fra_tokenizer.word_index.items()}\n","translated_sentence = ' '.join([reverse_fra_word_index.get(i, '') for i in predicted_ids])\n","print(\"Translated Sentence:\", translated_sentence)\n"]}]}